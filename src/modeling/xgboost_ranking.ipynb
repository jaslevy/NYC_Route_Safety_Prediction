{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lx/pn018tmx0pq2y3z58prkblww0000gn/T/ipykernel_10503/1751821100.py:3: DtypeWarning: Columns (30) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('../../static_data/processed/traffic_weather_intersections.csv')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('../../static_data/processed/traffic_weather_intersections.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in /Users/jlevy/.pyenv/versions/3.10.13/lib/python3.10/site-packages (3.0.0)\n",
      "Requirement already satisfied: scipy in /Users/jlevy/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from xgboost) (1.15.2)\n",
      "Requirement already satisfied: numpy in /Users/jlevy/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from xgboost) (2.2.5)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: scikit-learn in /Users/jlevy/.pyenv/versions/3.10.13/lib/python3.10/site-packages (1.6.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/jlevy/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /Users/jlevy/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from scikit-learn) (2.2.5)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /Users/jlevy/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from scikit-learn) (1.15.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/jlevy/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from scikit-learn) (3.6.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: numpy in /Users/jlevy/.pyenv/versions/3.10.13/lib/python3.10/site-packages (2.2.5)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade xgboost\n",
    "!pip install --upgrade scikit-learn\n",
    "!pip install --upgrade numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Negative Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lx/pn018tmx0pq2y3z58prkblww0000gn/T/ipykernel_10503/1868880825.py:9: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  date_range = pd.date_range(df['crash_date'].min(), df['crash_date'].max(), freq='H')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create datetime column\n",
    "df['datetime'] = pd.to_datetime(df['crash_date'].astype(str) + ' ' + df['crash_time'].astype(str), errors='coerce')\n",
    "\n",
    "# Get all unique intersections and datetimes\n",
    "all_intersections = df['nearest_intersection_id'].unique()\n",
    "date_range = pd.date_range(df['crash_date'].min(), df['crash_date'].max(), freq='H')\n",
    "all_combos = pd.MultiIndex.from_product([all_intersections, date_range], names=['nearest_intersection_id', 'datetime'])\n",
    "all_combos_df = all_combos.to_frame(index=False)\n",
    "\n",
    "# Mark where a crash occurred\n",
    "df['crash_occurred'] = 1\n",
    "crash_events = df[['nearest_intersection_id', 'datetime', 'crash_occurred']].drop_duplicates()\n",
    "\n",
    "# Merge to get all possible (intersection, datetime) and mark positives\n",
    "full_df = all_combos_df.merge(crash_events, on=['nearest_intersection_id', 'datetime'], how='left')\n",
    "full_df['crash_occurred'] = full_df['crash_occurred'].fillna(0).astype(int)\n",
    "\n",
    "# Sample negatives: for each intersection, sample N negatives per positive\n",
    "negatives = full_df[full_df['crash_occurred'] == 0].groupby('nearest_intersection_id').apply(\n",
    "    lambda x: x.sample(n=min(len(x), 2 * (full_df[(full_df['nearest_intersection_id'] == x.name) & (full_df['crash_occurred'] == 1)].shape[0])), random_state=42)\n",
    ").reset_index(drop=True)\n",
    "\n",
    "positives = full_df[full_df['crash_occurred'] == 1]\n",
    "final_df = pd.concat([positives, negatives], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "X = df[['nearest_intersection_id', 'hour', 'day_of_week', 'tavg', 'prcp', 'wspd']].copy()\n",
    "X['nearest_intersection_id'] = X['nearest_intersection_id'].astype('category')\n",
    "y = np.ones(len(df))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttest-ndcg:1.00000\n",
      "[1]\ttest-ndcg:1.00000\n",
      "[2]\ttest-ndcg:1.00000\n",
      "[3]\ttest-ndcg:1.00000\n",
      "[4]\ttest-ndcg:1.00000\n",
      "[5]\ttest-ndcg:1.00000\n",
      "[6]\ttest-ndcg:1.00000\n",
      "[7]\ttest-ndcg:1.00000\n",
      "[8]\ttest-ndcg:1.00000\n",
      "[9]\ttest-ndcg:1.00000\n",
      "[10]\ttest-ndcg:1.00000\n",
      "[11]\ttest-ndcg:1.00000\n",
      "[12]\ttest-ndcg:1.00000\n",
      "[13]\ttest-ndcg:1.00000\n",
      "[14]\ttest-ndcg:1.00000\n",
      "[15]\ttest-ndcg:1.00000\n",
      "[16]\ttest-ndcg:1.00000\n",
      "[17]\ttest-ndcg:1.00000\n",
      "[18]\ttest-ndcg:1.00000\n",
      "[19]\ttest-ndcg:1.00000\n",
      "[20]\ttest-ndcg:1.00000\n",
      "[21]\ttest-ndcg:1.00000\n",
      "[22]\ttest-ndcg:1.00000\n",
      "[23]\ttest-ndcg:1.00000\n",
      "[24]\ttest-ndcg:1.00000\n",
      "[25]\ttest-ndcg:1.00000\n",
      "[26]\ttest-ndcg:1.00000\n",
      "[27]\ttest-ndcg:1.00000\n",
      "[28]\ttest-ndcg:1.00000\n",
      "[29]\ttest-ndcg:1.00000\n",
      "[30]\ttest-ndcg:1.00000\n",
      "[31]\ttest-ndcg:1.00000\n",
      "[32]\ttest-ndcg:1.00000\n",
      "[33]\ttest-ndcg:1.00000\n",
      "[34]\ttest-ndcg:1.00000\n",
      "[35]\ttest-ndcg:1.00000\n",
      "[36]\ttest-ndcg:1.00000\n",
      "[37]\ttest-ndcg:1.00000\n",
      "[38]\ttest-ndcg:1.00000\n",
      "[39]\ttest-ndcg:1.00000\n",
      "[40]\ttest-ndcg:1.00000\n",
      "[41]\ttest-ndcg:1.00000\n",
      "[42]\ttest-ndcg:1.00000\n",
      "[43]\ttest-ndcg:1.00000\n",
      "[44]\ttest-ndcg:1.00000\n",
      "[45]\ttest-ndcg:1.00000\n",
      "[46]\ttest-ndcg:1.00000\n",
      "[47]\ttest-ndcg:1.00000\n",
      "[48]\ttest-ndcg:1.00000\n",
      "[49]\ttest-ndcg:1.00000\n",
      "[50]\ttest-ndcg:1.00000\n",
      "[51]\ttest-ndcg:1.00000\n",
      "[52]\ttest-ndcg:1.00000\n",
      "[53]\ttest-ndcg:1.00000\n",
      "[54]\ttest-ndcg:1.00000\n",
      "[55]\ttest-ndcg:1.00000\n",
      "[56]\ttest-ndcg:1.00000\n",
      "[57]\ttest-ndcg:1.00000\n",
      "[58]\ttest-ndcg:1.00000\n",
      "[59]\ttest-ndcg:1.00000\n",
      "[60]\ttest-ndcg:1.00000\n",
      "[61]\ttest-ndcg:1.00000\n",
      "[62]\ttest-ndcg:1.00000\n",
      "[63]\ttest-ndcg:1.00000\n",
      "[64]\ttest-ndcg:1.00000\n",
      "[65]\ttest-ndcg:1.00000\n",
      "[66]\ttest-ndcg:1.00000\n",
      "[67]\ttest-ndcg:1.00000\n",
      "[68]\ttest-ndcg:1.00000\n",
      "[69]\ttest-ndcg:1.00000\n",
      "[70]\ttest-ndcg:1.00000\n",
      "[71]\ttest-ndcg:1.00000\n",
      "[72]\ttest-ndcg:1.00000\n",
      "[73]\ttest-ndcg:1.00000\n",
      "[74]\ttest-ndcg:1.00000\n",
      "[75]\ttest-ndcg:1.00000\n",
      "[76]\ttest-ndcg:1.00000\n",
      "[77]\ttest-ndcg:1.00000\n",
      "[78]\ttest-ndcg:1.00000\n",
      "[79]\ttest-ndcg:1.00000\n",
      "[80]\ttest-ndcg:1.00000\n",
      "[81]\ttest-ndcg:1.00000\n",
      "[82]\ttest-ndcg:1.00000\n",
      "[83]\ttest-ndcg:1.00000\n",
      "[84]\ttest-ndcg:1.00000\n",
      "[85]\ttest-ndcg:1.00000\n",
      "[86]\ttest-ndcg:1.00000\n",
      "[87]\ttest-ndcg:1.00000\n",
      "[88]\ttest-ndcg:1.00000\n",
      "[89]\ttest-ndcg:1.00000\n",
      "[90]\ttest-ndcg:1.00000\n",
      "[91]\ttest-ndcg:1.00000\n",
      "[92]\ttest-ndcg:1.00000\n",
      "[93]\ttest-ndcg:1.00000\n",
      "[94]\ttest-ndcg:1.00000\n",
      "[95]\ttest-ndcg:1.00000\n",
      "[96]\ttest-ndcg:1.00000\n",
      "[97]\ttest-ndcg:1.00000\n",
      "[98]\ttest-ndcg:1.00000\n",
      "[99]\ttest-ndcg:1.00000\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "\n",
    "# Create DMatrix for train and test\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train, enable_categorical=True)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test, enable_categorical=True)\n",
    "\n",
    "# XGBoost parameters for ranking\n",
    "params = {\n",
    "    'objective': 'rank:pairwise',\n",
    "    'eval_metric': 'ndcg',\n",
    "    'eta': 0.1,\n",
    "    'max_depth': 6,\n",
    "    'tree_method': 'hist'\n",
    "}\n",
    "\n",
    "# Train the model and evaluate on test set\n",
    "model = xgb.train(\n",
    "    params,\n",
    "    dtrain,\n",
    "    num_boost_round=100,\n",
    "    evals=[(dtest, 'test')]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference: get risk scores for the test set\n",
    "risk_scores = model.predict(dtest)\n",
    "X_test = X_test.copy()\n",
    "X_test['risk_score'] = risk_scores\n",
    "\n",
    "# Example: Show top 10 riskiest intersections in the test set\n",
    "top_risk = X_test.sort_values('risk_score', ascending=False)\n",
    "print(top_risk[['nearest_intersection_id', 'hour', 'day_of_week', 'risk_score']].head(10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
